## Лабораторная работа 1

### Цель работы:

Научиться реализовывать один из алгоритмов глубокого обучения.

### Задание:

1. Скачайте [Car Datasets](https://drive.google.com/drive/folders/1pkudEBabqbXMxRTgfGQs3e0VqfTjtqWU)
2. Реализуйте нейронную сеть с оптимизатором согласно варианту задания. Архитектуру, указанную в варианте, необходимо
   реализовать с использованием Numpy и с Torch/Tensorflow/Jax
3. Оцените качество модели на тесте и сравните быстродействие реализованных вариантов.
4. Запустить обучение на классическом Adam и сравнить сходимость результатов с вариантом задания.
5. Сделайте отчёт в виде README на GitHub, там же должен быть выложен исходный код.

### Вариант 4:

* Модель: *AlexNet*
* Оптимизатор: *[AdaSmooth](https://arxiv.org/abs/2204.00825v1)*

### Структура проекта

[Nunes](../../nunes)

* [optimizers](../../nunes/optimizers) – реализация градиентного спуска
* [tensor](../../nunes/tensor) – обертка над numpy.ndarray в которую добавлена возможность считать градиенты
* [nn](../../nunes/nn) – основные слои нейронных сетей
    * [module](../../nunes/nn/module.py) – основной класс для конструирования собственных нейронных сетей
    * [losses](../../nunes/nn/losses.py) – реализация базовых функций ошибок
    * [activation](../../nunes/nn/activations.py) – реализация слоев функций активации
    * [layers](../../nunes/nn/layers) – реализация основных нейросетевых слоев
* [autograd](../../nunes/autograd) – реализация функций и графа вычислений
    * [functional](../../nunes/autograd/functional) – реализация функций над тензорами, включая их прямое и обратное
      распространение
        * [activation](../../nunes/autograd/functional/activations.py) – реализация функций активации
        * [vision](../../nunes/autograd/functional/vision.py) – реализация функций использующихся в CV
        * [common](../../nunes/autograd/functional/common.py) – реализация базовых математических функций
    * [graph](../../nunes/autograd/graph.py) – реализация графа вычислений и обратного распространения ошибки
* [models](../../nunes/models) – готовые нейросети, построенные из основных блоков

## Отчет:

1. Обучение сверточной нейросети происходило на наборе данных MNIST. Скрипт обучения самописной numpy модели
   лежит [здесь](mnist.ipynb).
   Программый код, позволяющий обучать нейросети на numpy лежит в папке [nunes](../../nunes). В рамках данного домашнего
   задания была реализована мини-библиотека позволяющая конструировать нейросети в torch-like стиле с построением
   динамического графа вычисления. Для этого был реализован [граф вычислений](../../nunes/autograd/graph.py), который
   позволяет автоматически распространять ошибку. В примере [example.ipynb](example.ipynb) представлен пример кода,
   позволяющий обучать нейросети с помощью nunes.
2. Как и ожидалось обучение простейшей нейросети на numpy заняло порядка 100 минут, чтобы добиться хорошего качества в
   задаче MNIST.
3. Код с обучением модели на pytorch представлен в файле [torch.ipynb](torch.ipynb).
4. Реализация AdaSmooth оптимизатора лежит в файле [adasmooth.py](adasmooth.py).
5. Как можно заметить оптимизатор AdaSmooth сходиться немного хуже, чем оптимизатор Adam. Скорость работы этих
   оптимизаторов сопоставима.

### Источники:

1. pytorch – https://pytorch.org/docs/stable/index.html
2. numpy – https://numpy.org/doc/