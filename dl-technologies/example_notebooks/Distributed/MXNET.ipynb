{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install mxnet"
      ],
      "metadata": {
        "id": "0HWdBP2Jr5Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install horovod"
      ],
      "metadata": {
        "id": "pE8eqBQLsBiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "import mxnet as mx\n",
        "import horovod.mxnet as hvd\n",
        "from mxnet import autograd, gluon, nd\n",
        "from mxnet.test_utils import download"
      ],
      "metadata": {
        "id": "zuHBFGIKl4hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "dtype = \"float32\"\n",
        "epochs = 5\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "no_cuda = True\n",
        "fp16_allreduce = False\n",
        "gradient_predivide_factor = 1.0\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "7sVSGYcbl8yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAMOxqMRl2BP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ac9603-2768-4404-aba9-9bb6ee64c3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 Batch 100] Training: accuracy=0.827969\n",
            "[Epoch 0 Batch 200] Training: accuracy=0.891484\n",
            "[Epoch 0 Batch 300] Training: accuracy=0.913594\n",
            "[Epoch 0 Batch 400] Training: accuracy=0.926250\n",
            "[Epoch 0 Batch 500] Training: accuracy=0.935438\n",
            "[Epoch 0 Batch 600] Training: accuracy=0.941536\n",
            "[Epoch 0 Batch 700] Training: accuracy=0.946607\n",
            "[Epoch 0 Batch 800] Training: accuracy=0.950820\n",
            "[Epoch 0 Batch 900] Training: accuracy=0.953681\n",
            "Epoch 0\tSpeed=1127.3894082024588 samples/s\tTime cost=53.19191360473633\n",
            "Epoch0\tTrain: accuracy=0.9548926093916755\tValidation: accuracy=0.983573717948718\n",
            "[Epoch 1 Batch 100] Training: accuracy=0.985156\n",
            "[Epoch 1 Batch 200] Training: accuracy=0.985547\n",
            "[Epoch 1 Batch 300] Training: accuracy=0.983958\n",
            "[Epoch 1 Batch 400] Training: accuracy=0.984570\n",
            "[Epoch 1 Batch 500] Training: accuracy=0.984688\n",
            "[Epoch 1 Batch 600] Training: accuracy=0.984766\n",
            "[Epoch 1 Batch 700] Training: accuracy=0.985000\n",
            "[Epoch 1 Batch 800] Training: accuracy=0.985469\n",
            "[Epoch 1 Batch 900] Training: accuracy=0.985521\n",
            "Epoch 1\tSpeed=1122.5489187855985 samples/s\tTime cost=53.42127990722656\n",
            "Epoch1\tTrain: accuracy=0.9857757470651014\tValidation: accuracy=0.9847756410256411\n",
            "[Epoch 2 Batch 100] Training: accuracy=0.990000\n",
            "[Epoch 2 Batch 200] Training: accuracy=0.989922\n",
            "[Epoch 2 Batch 300] Training: accuracy=0.989271\n",
            "[Epoch 2 Batch 400] Training: accuracy=0.989492\n",
            "[Epoch 2 Batch 500] Training: accuracy=0.989625\n",
            "[Epoch 2 Batch 600] Training: accuracy=0.989818\n",
            "[Epoch 2 Batch 700] Training: accuracy=0.990112\n",
            "[Epoch 2 Batch 800] Training: accuracy=0.990469\n",
            "[Epoch 2 Batch 900] Training: accuracy=0.990521\n",
            "Epoch 2\tSpeed=1128.152404996146 samples/s\tTime cost=53.15593862533569\n",
            "Epoch2\tTrain: accuracy=0.9907450640341515\tValidation: accuracy=0.9865785256410257\n",
            "[Epoch 3 Batch 100] Training: accuracy=0.993906\n",
            "[Epoch 3 Batch 200] Training: accuracy=0.993125\n",
            "[Epoch 3 Batch 300] Training: accuracy=0.992708\n",
            "[Epoch 3 Batch 400] Training: accuracy=0.992773\n",
            "[Epoch 3 Batch 500] Training: accuracy=0.992687\n",
            "[Epoch 3 Batch 600] Training: accuracy=0.992943\n",
            "[Epoch 3 Batch 700] Training: accuracy=0.993103\n",
            "[Epoch 3 Batch 800] Training: accuracy=0.993359\n",
            "[Epoch 3 Batch 900] Training: accuracy=0.993455\n",
            "Epoch 3\tSpeed=1126.824806502675 samples/s\tTime cost=53.218565702438354\n",
            "Epoch3\tTrain: accuracy=0.9936299359658485\tValidation: accuracy=0.9878806089743589\n",
            "[Epoch 4 Batch 100] Training: accuracy=0.995781\n",
            "[Epoch 4 Batch 200] Training: accuracy=0.995313\n",
            "[Epoch 4 Batch 300] Training: accuracy=0.994896\n",
            "[Epoch 4 Batch 400] Training: accuracy=0.995039\n",
            "[Epoch 4 Batch 500] Training: accuracy=0.994938\n",
            "[Epoch 4 Batch 600] Training: accuracy=0.995104\n",
            "[Epoch 4 Batch 700] Training: accuracy=0.995313\n",
            "[Epoch 4 Batch 800] Training: accuracy=0.995469\n",
            "[Epoch 4 Batch 900] Training: accuracy=0.995625\n",
            "Epoch 4\tSpeed=1143.874563135086 samples/s\tTime cost=52.425328731536865\n",
            "Epoch4\tTrain: accuracy=0.995781083244397\tValidation: accuracy=0.9893830128205128\n"
          ]
        }
      ],
      "source": [
        "# Function to get mnist iterator given a rank\n",
        "def get_mnist_iterator(rank, batch_size):\n",
        "    data_dir = \"data-%d\" % rank\n",
        "    if not os.path.isdir(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "    zip_file_path = download('http://data.mxnet.io/mxnet/data/mnist.zip',\n",
        "                             dirname=data_dir)\n",
        "    with zipfile.ZipFile(zip_file_path) as zf:\n",
        "        zf.extractall(data_dir)\n",
        "\n",
        "    input_shape = (1, 28, 28)\n",
        "\n",
        "    train_iter = mx.io.MNISTIter(\n",
        "        image=\"%s/train-images-idx3-ubyte\" % data_dir,\n",
        "        label=\"%s/train-labels-idx1-ubyte\" % data_dir,\n",
        "        input_shape=input_shape,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        flat=False,\n",
        "        num_parts=hvd.size(),\n",
        "        part_index=hvd.rank()\n",
        "    )\n",
        "\n",
        "    val_iter = mx.io.MNISTIter(\n",
        "        image=\"%s/t10k-images-idx3-ubyte\" % data_dir,\n",
        "        label=\"%s/t10k-labels-idx1-ubyte\" % data_dir,\n",
        "        input_shape=input_shape,\n",
        "        batch_size=batch_size,\n",
        "        flat=False,\n",
        "    )\n",
        "\n",
        "    return train_iter, val_iter\n",
        "\n",
        "\n",
        "# Function to define neural network\n",
        "def conv_nets():\n",
        "    net = gluon.nn.HybridSequential()\n",
        "    with net.name_scope():\n",
        "        net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
        "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
        "        net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
        "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
        "        net.add(gluon.nn.Flatten())\n",
        "        net.add(gluon.nn.Dense(512, activation=\"relu\"))\n",
        "        net.add(gluon.nn.Dense(10))\n",
        "    return net\n",
        "\n",
        "\n",
        "# Function to evaluate accuracy for a model\n",
        "def evaluate(model, data_iter, context):\n",
        "    data_iter.reset()\n",
        "    metric = mx.metric.Accuracy()\n",
        "    for _, batch in enumerate(data_iter):\n",
        "        data = batch.data[0].as_in_context(context)\n",
        "        label = batch.label[0].as_in_context(context)\n",
        "        output = model(data.astype(dtype, copy=False))\n",
        "        metric.update([label], [output])\n",
        "\n",
        "    return metric.get()\n",
        "\n",
        "\n",
        "# Initialize Horovod\n",
        "hvd.init()\n",
        "\n",
        "# Horovod: pin context to local rank\n",
        "context = mx.cpu(hvd.local_rank()) if no_cuda else mx.gpu(hvd.local_rank())\n",
        "num_workers = hvd.size()\n",
        "\n",
        "# Load training and validation data\n",
        "train_data, val_data = get_mnist_iterator(hvd.rank(), batch_size)\n",
        "\n",
        "# Build model\n",
        "model = conv_nets()\n",
        "model.cast(dtype)\n",
        "model.hybridize()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer_params = {'momentum': momentum,\n",
        "                    'learning_rate': lr * hvd.size()}\n",
        "opt = mx.optimizer.create('sgd', **optimizer_params)\n",
        "\n",
        "# Initialize parameters\n",
        "initializer = mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\",\n",
        "                             magnitude=2)\n",
        "model.initialize(initializer, ctx=context)\n",
        "\n",
        "# Horovod: fetch and broadcast parameters\n",
        "params = model.collect_params()\n",
        "if params is not None:\n",
        "    hvd.broadcast_parameters(params, root_rank=0)\n",
        "\n",
        "# Horovod: create DistributedTrainer, a subclass of gluon.Trainer\n",
        "compression = hvd.Compression.fp16 if fp16_allreduce else hvd.Compression.none\n",
        "trainer = hvd.DistributedTrainer(params, opt, compression=compression,\n",
        "                                 gradient_predivide_factor=gradient_predivide_factor)\n",
        "\n",
        "# Create loss function and train metric\n",
        "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "metric = mx.metric.Accuracy()\n",
        "\n",
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "    tic = time.time()\n",
        "    train_data.reset()\n",
        "    metric.reset()\n",
        "    for nbatch, batch in enumerate(train_data, start=1):\n",
        "        data = batch.data[0].as_in_context(context)\n",
        "        label = batch.label[0].as_in_context(context)\n",
        "        with autograd.record():\n",
        "            output = model(data.astype(dtype, copy=False))\n",
        "            loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        trainer.step(batch_size)\n",
        "        metric.update([label], [output])\n",
        "\n",
        "        if nbatch % 100 == 0:\n",
        "            name, acc = metric.get()\n",
        "            print('[Epoch %d Batch %d] Training: %s=%f' %\n",
        "                         (epoch, nbatch, name, acc))\n",
        "\n",
        "    if hvd.rank() == 0:\n",
        "        elapsed = time.time() - tic\n",
        "        speed = nbatch * batch_size * hvd.size() / elapsed\n",
        "        print(f'Epoch {epoch}\\tSpeed={speed} samples/s\\tTime cost={elapsed}')\n",
        "\n",
        "    # Evaluate model accuracy\n",
        "    _, train_acc = metric.get()\n",
        "    name, val_acc = evaluate(model, val_data, context)\n",
        "    if hvd.rank() == 0:\n",
        "        print(f'Epoch{epoch}\\tTrain: {name}={train_acc}\\tValidation: {name}={val_acc}')\n",
        "\n",
        "    if hvd.rank() == 0 and epoch == epochs - 1:\n",
        "        assert val_acc > 0.96, \"Achieved accuracy (%f) is lower than expected\\\n",
        "                                (0.96)\" % val_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2K2l4dRYnBSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}